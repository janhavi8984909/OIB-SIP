{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b28735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Twitter Sentiment Analysis - Model Training Notebook\n",
    "\n",
    "## 1. Import Libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"\n",
    "## 2. Load Processed Data\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/cleaned_twitter_data.csv')\n",
    "    print(\"Loaded processed data shape:\", df.shape)\n",
    "except:\n",
    "    # Fallback to original data with preprocessing\n",
    "    df = pd.read_csv('../Twitter_Data.csv')\n",
    "    # Apply preprocessing here if needed\n",
    "\n",
    "X = df['cleaned_text']\n",
    "y = df['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "\"\"\"\n",
    "## 3. Define Models and Parameters\n",
    "\"\"\"\n",
    "\n",
    "models = {\n",
    "    'Naive_Bayes': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', MultinomialNB())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'tfidf__max_features': [1000, 2000, 3000],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "            'clf__alpha': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Logistic_Regression': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', LogisticRegression(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'tfidf__max_features': [2000, 3000],\n",
    "            'tfidf__ngram_range': [(1, 2)],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__max_iter': [1000]\n",
    "        }\n",
    "    },\n",
    "    'Random_Forest': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'tfidf__max_features': [2000, 3000],\n",
    "            'clf__n_estimators': [100, 200],\n",
    "            'clf__max_depth': [10, 20, None],\n",
    "            'clf__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', SVC(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'tfidf__max_features': [2000, 3000],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "## 4. Model Training with Grid Search\n",
    "\"\"\"\n",
    "\n",
    "trained_models = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    pipeline = model_config['pipeline']\n",
    "    params = model_config['params']\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        params, \n",
    "        cv=5, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[model_name] = {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'cv_results': grid_search.cv_results_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Update best model\n",
    "    if grid_search.best_score_ > best_score:\n",
    "        best_score = grid_search.best_score_\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_model_name = model_name\n",
    "\n",
    "\"\"\"\n",
    "## 5. Model Evaluation\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model_info in trained_models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    model = model_info['model']\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "\"\"\"\n",
    "## 6. Model Comparison\n",
    "\"\"\"\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'CV_Score': trained_models[model_name]['best_score'],\n",
    "        'Test_Accuracy': result['accuracy']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test_Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, comparison_df['CV_Score'], width, label='CV Score', alpha=0.7)\n",
    "plt.bar(x + width/2, comparison_df['Test_Accuracy'], width, label='Test Accuracy', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, comparison_df['Model'], rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "## 7. Save Trained Models\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSaving trained models...\")\n",
    "\n",
    "for model_name, model_info in trained_models.items():\n",
    "    model = model_info['model']\n",
    "    filename = f\"../models/trained_models/{model_name}_model.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Saved {model_name} to {filename}\")\n",
    "\n",
    "# Save best model separately\n",
    "best_model_filename = \"../models/trained_models/best_model.pkl\"\n",
    "joblib.dump(best_model, best_model_filename)\n",
    "print(f\"\\nBest model ({best_model_name}) saved to {best_model_filename}\")\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "results_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_score': best_score,\n",
    "    'model_comparison': comparison_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open('../models/model_evaluation/training_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nModel training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
