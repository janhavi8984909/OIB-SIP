{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Twitter Sentiment Analysis - Evaluation & Visualization Notebook\n",
    "\n",
    "## 1. Import Libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_recall_curve, roc_curve)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import custom visualization classes\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from visualization import SentimentVisualizer\n",
    "from model_visualization import ModelPerformanceVisualizer\n",
    "\n",
    "\"\"\"\n",
    "## 2. Load Data and Models\n",
    "\"\"\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/processed/cleaned_twitter_data.csv')\n",
    "X_test = pd.read_csv('../data/processed/test_data.csv')['text']\n",
    "y_test = pd.read_csv('../data/processed/test_data.csv')['sentiment']\n",
    "\n",
    "# Load trained models\n",
    "models = {}\n",
    "model_files = {\n",
    "    'Naive_Bayes': '../models/trained_models/Naive_Bayes_model.pkl',\n",
    "    'Logistic_Regression': '../models/trained_models/Logistic_Regression_model.pkl',\n",
    "    'Random_Forest': '../models/trained_models/Random_Forest_model.pkl',\n",
    "    'SVM': '../models/trained_models/SVM_model.pkl'\n",
    "}\n",
    "\n",
    "for model_name, file_path in model_files.items():\n",
    "    try:\n",
    "        models[model_name] = joblib.load(file_path)\n",
    "        print(f\"Loaded {model_name}\")\n",
    "    except:\n",
    "        print(f\"Could not load {model_name}\")\n",
    "\n",
    "\"\"\"\n",
    "## 3. Model Predictions and Metrics\n",
    "\"\"\"\n",
    "\n",
    "from src.evaluation import ModelEvaluator\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Generate predictions and calculate metrics\n",
    "predictions = {}\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    result = evaluator.calculate_metrics(y_test, y_pred, y_prob, model_name)\n",
    "    predictions[model_name] = {'y_pred': y_pred, 'y_prob': y_prob}\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {result['metrics']['accuracy']:.4f}\")\n",
    "    print(f\"F1-Score: {result['metrics']['f1_macro']:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "## 4. Comprehensive Model Comparison\n",
    "\"\"\"\n",
    "\n",
    "# Generate comparison report\n",
    "comparison_df = evaluator.generate_comparison_report()\n",
    "\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot model comparison\n",
    "evaluator.plot_model_comparison()\n",
    "\n",
    "\"\"\"\n",
    "## 5. Confusion Matrix Visualization\n",
    "\"\"\"\n",
    "\n",
    "model_viz = ModelPerformanceVisualizer()\n",
    "model_viz.plot_confusion_matrix_comparison(\n",
    "    {name: {'pipeline': model} for name, model in models.items()},\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "## 6. Precision-Recall Comparison\n",
    "\"\"\"\n",
    "\n",
    "model_viz.plot_precision_recall_comparison(\n",
    "    {name: {'pipeline': model} for name, model in models.items()},\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "## 7. Data Visualization\n",
    "\"\"\"\n",
    "\n",
    "# Create comprehensive data visualizations\n",
    "data_viz = SentimentVisualizer(df)\n",
    "data_viz.create_comprehensive_dashboard()\n",
    "data_viz.create_wordclouds_grid()\n",
    "\n",
    "\"\"\"\n",
    "## 8. Feature Importance Analysis\n",
    "\"\"\"\n",
    "\n",
    "# Get the best model for feature importance analysis\n",
    "best_model_name = comparison_df.index[0]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "if hasattr(best_model.named_steps['clf'], 'coef_'):\n",
    "    print(f\"\\nFeature Importance Analysis for {best_model_name}\")\n",
    "    \n",
    "    # Get feature names and coefficients\n",
    "    feature_names = best_model.named_steps['tfidf'].get_feature_names_out()\n",
    "    coefficients = best_model.named_steps['clf'].coef_\n",
    "    \n",
    "    # Plot feature importance for each class\n",
    "    sentiment_names = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    for i, sentiment in enumerate([-1, 0, 1]):\n",
    "        # Get top features for this sentiment\n",
    "        sentiment_coef = coefficients[i] if len(coefficients) > 1 else coefficients[0]\n",
    "        top_indices = np.argsort(sentiment_coef)[-10:]\n",
    "        top_features = feature_names[top_indices]\n",
    "        top_scores = sentiment_coef[top_indices]\n",
    "        \n",
    "        axes[i].barh(range(len(top_features)), top_scores, color=['red', 'blue', 'green'][i])\n",
    "        axes[i].set_yticks(range(len(top_features)))\n",
    "        axes[i].set_yticklabels(top_features)\n",
    "        axes[i].set_title(f'Top Features - {sentiment_names[i]}')\n",
    "        axes[i].set_xlabel('Coefficient Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "## 9. Error Analysis\n",
    "\"\"\"\n",
    "\n",
    "# Analyze misclassifications for the best model\n",
    "best_model_predictions = predictions[best_model_name]['y_pred']\n",
    "misclassified = X_test[y_test != best_model_predictions]\n",
    "misclassified_actual = y_test[y_test != best_model_predictions]\n",
    "misclassified_pred = best_model_predictions[y_test != best_model_predictions]\n",
    "\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'text': misclassified,\n",
    "    'actual': misclassified_actual,\n",
    "    'predicted': misclassified_pred\n",
    "})\n",
    "\n",
    "print(f\"\\nMisclassification Analysis for {best_model_name}:\")\n",
    "print(f\"Total misclassified: {len(misclassified_df)}\")\n",
    "print(f\"Misclassification rate: {len(misclassified_df)/len(X_test):.4f}\")\n",
    "\n",
    "print(\"\\nSample misclassifications:\")\n",
    "print(misclassified_df.head(10))\n",
    "\n",
    "\"\"\"\n",
    "## 10. Interactive Visualizations\n",
    "\"\"\"\n",
    "\n",
    "# Create interactive dashboard\n",
    "try:\n",
    "    interactive_viz = InteractiveVisualizations(df)\n",
    "    interactive_viz.create_interactive_sentiment_dashboard()\n",
    "    interactive_viz.create_interactive_word_cloud()\n",
    "except:\n",
    "    print(\"Interactive visualizations require plotly\")\n",
    "\n",
    "\"\"\"\n",
    "## 11. Save Evaluation Results\n",
    "\"\"\"\n",
    "\n",
    "# Save detailed evaluation results\n",
    "evaluation_results = {\n",
    "    'model_comparison': comparison_df.to_dict(),\n",
    "    'misclassification_analysis': {\n",
    "        'total_misclassified': len(misclassified_df),\n",
    "        'misclassification_rate': len(misclassified_df)/len(X_test),\n",
    "        'sample_misclassifications': misclassified_df.head(20).to_dict('records')\n",
    "    },\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'accuracy': comparison_df.loc[best_model_name, 'accuracy'],\n",
    "        'f1_score': comparison_df.loc[best_model_name, 'f1_macro']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../models/model_evaluation/detailed_evaluation.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(\"\\nEvaluation completed successfully!\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Best accuracy: {comparison_df.loc[best_model_name, 'accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
